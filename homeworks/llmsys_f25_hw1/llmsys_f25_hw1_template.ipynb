{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFcC_FHE-Lx7"
      },
      "source": [
        "# **Assignment 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUDDzNFV-Yr7"
      },
      "source": [
        "---\n",
        "## üìö **Introduction**\n",
        "This template notebook is designed to guide you through Assignment 1 of the LLM System course. Follow the steps to set up your environment, implement the required CUDA kernels, and test your code.\n",
        "\n",
        "### üöÄ **Goal of the Assignment**\n",
        "You will implement high-performance CUDA kernels for tensor operations and integrate them with the MiniTorch framework. You will implement low-level operators in CUDA C++ and connect them to Python through the CUDA backend. This assignment focuses on parallel computing concepts and GPU acceleration techniques.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkQ21W7h-eCq"
      },
      "source": [
        "## ‚öôÔ∏è **Environment Setup**\n",
        "First, ensure that you have changed the runtime to **T4 GPU**. Run the following commands to set up your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXnYt1dB-Nb4"
      },
      "outputs": [],
      "source": [
        "# Clone the starter code repository\n",
        "!git clone https://github.com/llmsystem/llmsys_f25_hw1.git\n",
        "%cd llmsys_f25_hw1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_hSUoH2-iGn"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!python -m pip install -r requirements.txt\n",
        "!python -m pip install -r requirements.extra.txt\n",
        "!python -m pip install -Ue ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORwEm87d-wdz"
      },
      "source": [
        "---\n",
        "\n",
        "## üîß **CUDA Kernel Compilation**\n",
        "You will need to compile the CUDA kernels for this assignment. Run the following command to create the necessary directory and compile the CUDA files.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk8IZjW9-sci"
      },
      "outputs": [],
      "source": [
        "# Compile CUDA kernels\n",
        "!mkdir -p minitorch/cuda_kernels\n",
        "!nvcc -o minitorch/cuda_kernels/combine.so --shared src/combine.cu -Xcompiler -fPIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4qk5ZS0_HcH"
      },
      "source": [
        "## üìã **Assignment Sections**\n",
        "\n",
        "### üßÆ **Problem 1: Map Operation CUDA Kernel + Integration (15 points)**\n",
        "**Goal:** Implement the CUDA kernel for element-wise map operations and integrate it with the MiniTorch framework.\n",
        "\n",
        "The map operation applies a unary function to every element of an input tensor, producing a new tensor with the same shape. For example, applying `f(x) = x¬≤` to tensor `[1, 2, 3]` yields `[1, 4, 9]`.\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `src/combine.cu`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_1` and `END ASSIGN2_1`.\n",
        "3. Implement the `mapKernel` function.\n",
        "\n",
        "**Key Points:**\n",
        "- Each thread should process one element of the output tensor\n",
        "- Use thread and block indices to calculate the global thread ID\n",
        "- Ensure proper bounds checking to avoid out-of-bounds memory access\n",
        "- Consider the stride-based indexing for multidimensional tensors\n",
        "\n",
        "**Testing:**\n",
        "Run the following command to test your implementation.\n",
        "\n",
        "```python\n",
        "!python -m pytest -l -v -k \"cuda_one_args\"\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a0b3024-2uA"
      },
      "outputs": [],
      "source": [
        "# Problem 1: Map Operation CUDA Kernel Tests\n",
        "\n",
        "# TODO: Implement the mapKernel function in src/combine.cu\n",
        "# Make sure to recompile CUDA kernels before testing:\n",
        "# !nvcc -o minitorch/cuda_kernels/combine.so --shared src/combine.cu -Xcompiler -fPIC\n",
        "\n",
        "!python -m pytest -l -v -k \"cuda_one_args\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_CEvR3R_hP8"
      },
      "source": [
        "---\n",
        "\n",
        "### üöÄ **Problem 2: Zip Operation CUDA Kernel + Integration (20 points)**\n",
        "\n",
        "**Goal:** Implement the CUDA kernel for element-wise zip operations and integrate it with the framework.\n",
        "\n",
        "This operation applies a binary function to corresponding elements from two input tensors, producing a new tensor with the same shape. For example, applying addition `f(x,y) = x + y` to tensors `[1, 2, 3]` and `[4, 5, 6]` yields `[5, 7, 9]`.\n",
        "\n",
        "#### **Part A: Implement zipKernel (15 points)**\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `src/combine.cu`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_2` and `END ASSIGN2_2`.\n",
        "3. Implement the `zipKernel` function.\n",
        "\n",
        "**Key Points:**\n",
        "- Each thread processes one element from each input tensor\n",
        "- Both input tensors should have the same shape or be broadcastable\n",
        "- Handle stride-based indexing for both input tensors\n",
        "- Ensure proper bounds checking\n",
        "\n",
        "#### **Part B: Integrate Zip Operation (5 points)**\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `minitorch/cuda_kernel_ops.py`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_2_INTEGRATION` and `END ASSIGN2_2_INTEGRATION`.\n",
        "3. Implement the `zip` function in the `CudaKernelOps` class.\n",
        "\n",
        "**Testing:**\n",
        "```bash\n",
        "!python -m pytest -l -v -k \"cuda_two_args\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKJGrLp4_gkk"
      },
      "outputs": [],
      "source": [
        "!nvcc -o minitorch/cuda_kernels/combine.so --shared src/combine.cu -Xcompiler -fPIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTKV3nYqbRrk"
      },
      "outputs": [],
      "source": [
        "# Problem 2: Zip Operation CUDA Kernel Tests\n",
        "\n",
        "# TODO: \n",
        "# 1. Implement the zipKernel function in src/combine.cu\n",
        "# 2. Implement the zip integration in minitorch/cuda_kernel_ops.py\n",
        "# Make sure to recompile CUDA kernels before testing:\n",
        "# !nvcc -o minitorch/cuda_kernels/combine.so --shared src/combine.cu -Xcompiler -fPIC\n",
        "\n",
        "!python -m pytest -l -v -k \"cuda_two_args\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zovjezvvccIr"
      },
      "source": [
        "### üß† **Problem 3: Reduce Operation CUDA Kernel + Integration (20 points)**\n",
        "\n",
        "**Goal:** Implement the CUDA kernel for reduction operations and integrate it with the framework.\n",
        "\n",
        "This operation aggregates elements along a specified dimension of a tensor using a binary function, producing a tensor with reduced dimensionality. For example, reducing tensor `[[1, 2, 3], [4, 5, 6]]` along dimension 1 with sum yields `[6, 15]`.\n",
        "\n",
        "#### **Part A: Implement reduceKernel (15 points)**\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `src/combine.cu`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_3` and `END ASSIGN2_3`.\n",
        "3. Implement the `reduceKernel` function.\n",
        "\n",
        "**Key Points - Basic Reduction:**\n",
        "- A simple way to parallelize the reduce function is to have every reduced element in the output calculated individually in each block\n",
        "- Each block takes care of computing one output element\n",
        "- It's important to think about how to calculate the step across the data to be reduced based on `reduce_dim` and `strides`\n",
        "\n",
        "#### **Part B: Integrate Reduce Operation (5 points)**\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `minitorch/cuda_kernel_ops.py`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_3_INTEGRATION` and `END ASSIGN2_3_INTEGRATION`.\n",
        "3. Implement the `reduce` function in the `CudaKernelOps` class.\n",
        "\n",
        "**Testing:**\n",
        "```bash\n",
        "!python -m pytest -l -v -k \"cuda_reduce\"\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gv2pg0fcWP3"
      },
      "outputs": [],
      "source": [
        "# Problem 3: Reduce Operation CUDA Kernel Tests\n",
        "\n",
        "# TODO: \n",
        "# 1. Implement the reduceKernel function in src/combine.cu\n",
        "# 2. Implement the reduce integration in minitorch/cuda_kernel_ops.py\n",
        "# Make sure to recompile CUDA kernels before testing:\n",
        "# !nvcc -o minitorch/cuda_kernels/combine.so --shared src/combine.cu -Xcompiler -fPIC\n",
        "\n",
        "!python -m pytest -l -v -k \"cuda_reduce\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKz9Ja8Nclln"
      },
      "source": [
        "### üìà **Problem 4: Matrix Multiplication CUDA Kernel + Integration (25 points)**\n",
        "\n",
        "**Goal:** Implement the CUDA kernel for matrix multiplication and integrate it with the framework.\n",
        "\n",
        "This is one of the most important operations in deep learning and offers significant opportunities for optimization.\n",
        "\n",
        "#### **Part A: Implement MatrixMultiplyKernel (20 points)**\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `src/combine.cu`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_4` and `END ASSIGN2_4`.\n",
        "3. Implement the `MatrixMultiplyKernel` function.\n",
        "\n",
        "**Key Points - Simple Parallelization:**\n",
        "- A simple way to parallelize matrix multiplication is to have every element in the output matrix calculated individually in each thread\n",
        "- Each thread computes one output element by performing the dot product of the corresponding row and column\n",
        "- Use proper indexing to handle 2D thread blocks and memory access patterns\n",
        "\n",
        "#### **Part B: Integrate Matrix Multiplication (5 points)**\n",
        "\n",
        "üîß **Instructions:**\n",
        "1. Navigate to `minitorch/cuda_kernel_ops.py`.\n",
        "2. Locate the placeholders marked with `BEGIN ASSIGN2_4_INTEGRATION` and `END ASSIGN2_4_INTEGRATION`.\n",
        "3. Implement the `matrix_multiply` function in the `CudaKernelOps` class.\n",
        "\n",
        "**Testing:**\n",
        "```bash\n",
        "!python -m pytest -l -v -k \"cuda_matmul\"\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvvFJTTmcqcS"
      },
      "outputs": [],
      "source": [
        "# Problem 4: Matrix Multiplication CUDA Kernel Tests\n",
        "\n",
        "# TODO: \n",
        "# 1. Implement the MatrixMultiplyKernel function in src/combine.cu\n",
        "# 2. Implement the matrix_multiply integration in minitorch/cuda_kernel_ops.py\n",
        "# Make sure to recompile CUDA kernels before testing:\n",
        "# !nvcc -o minitorch/cuda_kernels/combine.so --shared src/combine.cu -Xcompiler -fPIC\n",
        "\n",
        "!python -m pytest -l -v -k \"cuda_matmul\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ **Problem 5: Final Integration Test (5 points)**\n",
        "\n",
        "**Goal:** Verify that all CUDA kernels work together correctly with comprehensive test cases.\n",
        "\n",
        "After correctly implementing all functions in Problems 1-4, you should be able to pass all CUDA tests. This integration test includes more comprehensive test cases than the individual problem tests.\n",
        "\n",
        "**Testing:**\n",
        "Run the following command to test all CUDA implementations together:\n",
        "\n",
        "```bash\n",
        "!python -m pytest -l -v -k \"cuda\"\n",
        "```\n",
        "\n",
        "**Note:** If you pass the previous problem tests but fail here, please review your implementations for edge cases and ensure proper integration between kernels.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Problem 5: Final Integration Test\n",
        "\n",
        "# Run comprehensive CUDA tests to verify all implementations work together\n",
        "!python -m pytest -l -v -k \"cuda\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB62HJR-d_3N"
      },
      "source": [
        "---\n",
        "\n",
        "### üíæ **Submit Your Assignment: Create a ZIP File for Submission**\n",
        "\n",
        "Run the following code to create a `llmsys_f25_hw1.zip` file, which you can download and upload to Canvas:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üìã **Instructions for Submission:**\n",
        "1. **Run the cell below.**  \n",
        "   - This will generate a `llmsys_f25_hw1.zip` file containing your entire project.\n",
        "2. **Click the download link** that appears after the cell finishes running.\n",
        "3. **Upload the downloaded ZIP file to Canvas.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MpllLJl_d8TW",
        "outputId": "de44b4a8-dce6-4d2d-f708-73dc85a264ce"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c590d498-309a-47b1-9ca0-bf4e9fb09eb0\", \"llmsys_s25_hw1.zip\", 22)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Define the directory to zip\n",
        "dir_to_zip = \"llmsys_f25_hw1\"\n",
        "\n",
        "# Create a zip file\n",
        "output_filename = f\"{dir_to_zip}.zip\"\n",
        "shutil.make_archive(dir_to_zip, 'zip', dir_to_zip)\n",
        "\n",
        "# Provide a download link\n",
        "from google.colab import files\n",
        "files.download(output_filename)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
