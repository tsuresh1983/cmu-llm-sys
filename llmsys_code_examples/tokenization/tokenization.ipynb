{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import defaultdict \n",
    "import string\n",
    "\n",
    "def get_init_vocab(data): \n",
    "    \"\"\" \n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency  \n",
    "    count in the data. \n",
    "    Args: \n",
    "        data: raw text with line breaks\n",
    "        \n",
    "    Returns: \n",
    "        (vocab, tokens) tuple, \n",
    "          vocab is a dictionary mapping space delimited characters to count (e.g. {'a b c </w>': 5})\n",
    "          tokens is a set of basic characters. \n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    tokens = set()\n",
    "    tokens.add('</w>')\n",
    "    for line in data: \n",
    "        for word in line.split(): \n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "            tokens.update(list(word))\n",
    "    return vocab, tokens \n",
    "  \n",
    "def count_cooccurance(vocab): \n",
    "    \"\"\" \n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a  \n",
    "    dictionary of tuples representing the frequency count of pairs of characters  \n",
    "    in the vocabulary. \n",
    "    Args:\n",
    "        vocab: a dictionary mapping space-delimited tokens to count (e.g. {'a b c </w>': 5})\n",
    "        \n",
    "    Returns: \n",
    "        a dictionary mapping a tuple of tokens to count\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int) \n",
    "    for word, freq in vocab.items(): \n",
    "        chars = word.split() # split the word by any white space\n",
    "        for i in range(len(chars)-1): \n",
    "            pairs[chars[i], chars[i+1]] += freq \n",
    "    return pairs\n",
    "  \n",
    "def merge_vocab(token_pair, vocab_in): \n",
    "    \"\"\" \n",
    "    Given a pair of tokens and a vocabulary, returns a new vocabulary with the  \n",
    "    pair of tokens merged together wherever they appear. \n",
    "    \n",
    "    e.g. merge_vocab(('a', 'b'), {'a b c </w>': 5})\n",
    "    returns {'ab c </w>': 5}\n",
    "    \n",
    "    Args: \n",
    "        token_pair: a tuple of two tokens\n",
    "        vocab_in: a dictionary mapping space-delimited tokens to count (e.g. {'a b c </w>': 5})\n",
    "        \n",
    "    Returns: \n",
    "        a dictionary mapping space-delimited tokens to count (e.g. {'a b c </w>': 5})\n",
    "    \"\"\"\n",
    "    vocab_out = defaultdict(int)  \n",
    "    bigram = re.escape(' '.join(token_pair)) \n",
    "    new_token = ''.join(token_pair)\n",
    "    # search for every occurance of bigram (token pairs with a space), \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in vocab_in:\n",
    "        # replace the bigram (with space), with the new merged token (the concanated pair)\n",
    "        w_out = p.sub(new_token, word)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out\n",
    "\n",
    "  \n",
    "def byte_pair_encoding(data, n): \n",
    "    \"\"\" \n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs \n",
    "    of characters found in the vocabulary of the input data. \n",
    "    \n",
    "    Args: \n",
    "        data: raw text\n",
    "        n: number of merge opperations\n",
    "    \n",
    "    Returns: \n",
    "        a list of tokens\n",
    "        a dictionary mapping token to index (starting from 0)\n",
    "    \"\"\"\n",
    "    vocab, init_tokens = get_init_vocab(data)\n",
    "    tokens = list(init_tokens)\n",
    "    for i in range(n): \n",
    "        pairs = count_cooccurance(vocab) \n",
    "        best_pair = max(pairs, key=pairs.get) \n",
    "        new_token = ''.join(best_pair)\n",
    "        tokens.append(new_token)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        print('step {}: merging \\\"{}\\\" and \\\"{}\\\"'.format(i+1, best_pair[0], best_pair[1]))\n",
    "    token_to_ids = dict([(tk, id) for id, tk in enumerate(tokens)])\n",
    "    return tokens, token_to_ids\n",
    "\n",
    "def tokenize(data, token_dict):\n",
    "    \"\"\"\n",
    "    split the data into tokens and map into index. \n",
    "    It applies greedy split to text with longest matching.\n",
    "    \n",
    "    e.g. \n",
    "    tokenize(\"spiderman\", {'spider':0,'man': 1})\n",
    "    will return\n",
    "     [0, 1]\n",
    "        \n",
    "    Args: \n",
    "        data: raw text\n",
    "        token_dict: a dictionary mapping from token to id\n",
    "        \n",
    "    Returns: \n",
    "        a list of ids\n",
    "        \n",
    "    \"\"\"\n",
    "    encoded_ids = []\n",
    "    for line in data: \n",
    "        for word in line.split():\n",
    "            word = word + '</w>'\n",
    "            last_idx = 0\n",
    "            idx = len(word)\n",
    "            while idx > last_idx:\n",
    "                whole_word = word[last_idx:idx]\n",
    "                if whole_word in token_dict:\n",
    "                    encoded_ids.append(token_dict[whole_word])\n",
    "                    last_idx = idx\n",
    "                    idx = len(word)\n",
    "                else:\n",
    "                    idx = idx - 1\n",
    "    return encoded_ids\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: merging \"e\" and \"r\"\n",
      "step 2: merging \"s\" and \"</w>\"\n",
      "step 3: merging \"e\" and \"</w>\"\n",
      "step 4: merging \"e\" and \"n\"\n",
      "step 5: merging \"d\" and \"</w>\"\n",
      "step 6: merging \"h\" and \"er\"\n",
      "step 7: merging \"en\" and \"t\"\n",
      "step 8: merging \"e\" and \"d</w>\"\n",
      "step 9: merging \",\" and \"</w>\"\n",
      "step 10: merging \"her\" and \"</w>\"\n",
      "step 11: merging \"n\" and \"</w>\"\n",
      "step 12: merging \"p\" and \"a\"\n",
      "step 13: merging \"pa\" and \"r\"\n",
      "step 14: merging \"par\" and \"ent\"\n",
      "step 15: merging \"en\" and \"</w>\"\n",
      "step 16: merging \"h\" and \"e</w>\"\n",
      "step 17: merging \"a\" and \"s</w>\"\n",
      "step 18: merging \"s\" and \"e\"\n",
      "step 19: merging \"e\" and \"a\"\n",
      "step 20: merging \"i\" and \"t\"\n",
      "The bpe tokens are: \n",
      "W: 0\n",
      "c: 1\n",
      "[: 2\n",
      "b: 3\n",
      "w: 4\n",
      "a: 5\n",
      "s: 6\n",
      "d: 7\n",
      "m: 8\n",
      "T: 9\n",
      "f: 10\n",
      "y: 11\n",
      "</w>: 12\n",
      "k: 13\n",
      "t: 14\n",
      "H: 15\n",
      "o: 16\n",
      "': 17\n",
      "O: 18\n",
      "p: 19\n",
      "D: 20\n",
      "e: 21\n",
      "B: 22\n",
      "n: 23\n",
      "i: 24\n",
      "]: 25\n",
      "h: 26\n",
      ",: 27\n",
      "u: 28\n",
      "6: 29\n",
      "l: 30\n",
      "g: 31\n",
      "v: 32\n",
      "r: 33\n",
      "er: 34\n",
      "s</w>: 35\n",
      "e</w>: 36\n",
      "en: 37\n",
      "d</w>: 38\n",
      "her: 39\n",
      "ent: 40\n",
      "ed</w>: 41\n",
      ",</w>: 42\n",
      "her</w>: 43\n",
      "n</w>: 44\n",
      "pa: 45\n",
      "par: 46\n",
      "parent: 47\n",
      "en</w>: 48\n",
      "he</w>: 49\n",
      "as</w>: 50\n",
      "se: 51\n",
      "ea: 52\n",
      "it: 53\n",
      "The ids of the tokenized sequence are: \n",
      "[22, 34, 8, 5, 23, 17, 35, 47, 35, 7, 24, 32, 16, 33, 1, 41, 4, 26, 48, 49, 4, 50, 51, 32, 48, 9, 39, 52, 10, 14, 34, 42, 49, 6, 19, 30, 53, 12, 14, 24, 8, 36, 3, 21, 14, 4, 21, 48, 52, 1, 26, 12, 47, 17, 35, 26, 16, 28, 51, 26, 16, 30, 38, 28, 23, 14, 24, 30, 12, 49, 40, 34, 41, 1, 16, 30, 30, 21, 31, 36, 2, 29, 25, 12, 15, 24, 35, 10, 5, 14, 43, 33, 21, 30, 16, 1, 5, 14, 41, 14, 16, 12, 20, 5, 30, 30, 50, 10, 16, 33, 12, 5, 12, 19, 16, 6, 53, 24, 16, 44, 50, 5, 12, 30, 16, 3, 3, 11, 24, 6, 14, 12, 16, 44, 3, 21, 26, 5, 30, 10, 12, 16, 10, 12, 10, 16, 16, 7, 51, 33, 32, 24, 1, 36, 3, 28, 6, 24, 23, 21, 6, 51, 6, 42, 4, 26, 24, 30, 36, 26, 24, 35, 8, 16, 14, 43, 8, 16, 32, 41, 3, 5, 1, 13, 12, 24, 44, 4, 53, 26, 12, 43, 47, 35, 24, 44, 0, 16, 16, 6, 14, 34, 42, 18, 26, 24, 16, 42, 5, 23, 38, 3, 21, 1, 5, 8, 36, 5, 12, 14, 52, 1, 43, 14, 39, 36]\n",
      "\n",
      "The sequence corresponding to ids is: \n",
      "B er m a n ' s</w> parent s</w> d i v o r c ed</w> w h en</w> he</w> w as</w> se v en</w> T her ea f t er ,</w> he</w> s p l it </w> t i m e</w> b e t w e en</w> ea c h </w> parent ' s</w> h o u se h o l d</w> u n t i l </w> he</w> ent er ed</w> c o l l e g e</w> [ 6 ] </w> H i s</w> f a t her</w> r e l o c a t ed</w> t o </w> D a l l as</w> f o r </w> a </w> p o s it i o n</w> as</w> a </w> l o b b y i s t </w> o n</w> b e h a l f </w> o f </w> f o o d se r v i c e</w> b u s i n e s se s ,</w> w h i l e</w> h i s</w> m o t her</w> m o v ed</w> b a c k </w> i n</w> w it h </w> her</w> parent s</w> i n</w> W o o s t er ,</w> O h i o ,</w> a n d</w> b e c a m e</w> a </w> t ea c her</w> t her e</w>\n"
     ]
    }
   ],
   "source": [
    "# Example usage: \n",
    "corpus = '''Berman's parents divorced when he was seven. \n",
    "Thereafter, he split time between each parent's household until he entered college.[6] \n",
    "His father relocated to Dallas for a position as a lobbyist on behalf of foodservice businesses, \n",
    "while his mother moved back in with her parents in Wooster, Ohio, and became a teacher there'''\n",
    "data = corpus.split('.') \n",
    "  \n",
    "n = 20 # number of merge operations\n",
    "id_to_tokens, token_to_ids = byte_pair_encoding(data, n)\n",
    "\n",
    "token_ids = tokenize(data, token_to_ids)\n",
    "\n",
    "print(\"The bpe tokens are: \")\n",
    "for tk, tid in token_to_ids.items():\n",
    "    print(\"{}: {}\".format(tk, tid))\n",
    "\n",
    "print(\"The ids of the tokenized sequence are: \")\n",
    "print(token_ids)\n",
    "print()\n",
    "print(\"The sequence corresponding to ids is: \")\n",
    "print(' '.join(id_to_tokens[tid] for tid in token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
